{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e533aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets \n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from safetensors import safe_open\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6ae418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/vision/u/eatang/ml_superb/eighth_version/'\n",
    "languages = []\n",
    "\n",
    "sources = os.listdir(dataset_path)\n",
    "\n",
    "for source in sources:\n",
    "    if source[0] != \".\":\n",
    "        languages.extend(os.listdir(os.path.join(dataset_path, source)))\n",
    "        \n",
    "languages = set([x for x in languages if '.' not in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5a2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "all_paths = {}\n",
    "all_sentences = {}\n",
    "for duration in [\"10min\", \"1h\"]:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        language_to_paths = defaultdict(list)\n",
    "        language_to_sentences = defaultdict(list)\n",
    "        for language in languages:\n",
    "            for source in sources:\n",
    "                source_lang_path = os.path.join(dataset_path, source, language)\n",
    "                if os.path.exists(os.path.join(source_lang_path, f'transcript_{duration}_{split}.txt')):\n",
    "                    with open(os.path.join(source_lang_path, f'transcript_{duration}_{split}.txt'), 'r') as file:\n",
    "                        lines = [line.rstrip() for line in file]\n",
    "                        sentences = []\n",
    "                        paths = []\n",
    "                        for line in lines:\n",
    "                            sentence = \" \".join(re.split(r'[ \\t]+', line)[2:])\n",
    "                            sentence = remove_punctuation(sentence).lower().strip()\n",
    "                            if len(sentence) <= 1:\n",
    "                                continue\n",
    "                            if len(re.split(r'[ \\t]+', line)[0]) > 0:\n",
    "                                sentences.append(sentence)\n",
    "                                paths.append(os.path.join(source_lang_path, 'wav', re.split(r'[ \\t]+', line)[0] + '.wav'))\n",
    "\n",
    "                        language_to_paths[language].extend(paths)\n",
    "                        language_to_sentences[language].extend(sentences)\n",
    "        all_paths[duration + split] = language_to_paths\n",
    "        all_sentences[duration + split] = language_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9617fc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['glg', 'kab', 'lav', 'pan', 'ind', 'deu', 'tgk', 'ben', 'slv', 'yor', 'vie', 'orm', 'slk', 'msa', 'hin', 'ven', 'uig', 'nld', 'ast', 'kan', 'mon', 'khm', 'rus', 'mlt', 'srp', 'fin', 'sun', 'por', 'hun', 'est', 'tso', 'tur', 'tos', 'urd', 'uzb', 'yue', 'sna', 'kir', 'bul', 'org_jpn', 'cnh', 'ina', 'fil', 'ara', 'heb', 'wol', 'ceb', 'mya', 'fas', 'zul', 'aze', 'ltz', 'ori', 'xty', 'eus', 'ron', 'ell', 'pus', 'swa', 'nep', 'tha', 'nno', 'bre', 'sin', 'hsb', 'isl', 'hye', 'umb', 'epo', 'kmr', 'nan', 'pol', 'ssw', 'mhr', 'kor', 'sah', 'afr', 'amh', 'fra', 'cat', 'nya', 'snd', 'hau', 'asm', 'lug', 'kaz', 'mrj', 'lit', 'tsn', 'myv', 'ibo', 'hrv', 'luo', 'ckb', 'grn', 'sot', 'kat', 'bel', 'tam', 'lao', 'guj', 'div', 'mal', 'xho', 'ful', 'kea', 'ita', 'frr', 'cym', 'lga', 'tok', 'ces', 'cmn', 'eng', 'gle', 'bos', 'mri', 'kin', 'jpn', 'tel', 'chv', 'dan', 'ukr', 'bas', 'oci', 'jav', 'som', 'lin', 'nso', 'skr', 'ory', 'swe', 'bak', 'abk', 'mar', 'tat', 'mkd', 'nob', 'nbl', 'spa', 'kam'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_paths['10mintrain'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca382554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor,WhisperForConditionalGeneration, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from jiwer import wer\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_audio(path):\n",
    "    speech, _ = torchaudio.load(path)\n",
    "    return speech.squeeze().numpy()\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(batch):\n",
    "    audio = [load_audio(path) for path in batch[\"audio\"]]\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\").to(\"cuda\")\n",
    "    labels = processor.tokenizer(text=batch[\"sentence\"], return_tensors=\"pt\", padding=True).input_ids\n",
    "    inputs[\"labels\"] = labels.to(\"cuda\")\n",
    "    return inputs\n",
    "\n",
    "# Function to decode model predictions\n",
    "def decode_predictions(pred_ids):\n",
    "    pred_ids = pred_ids.cpu().numpy()\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    return pred_str\n",
    "\n",
    "# Evaluate WER\n",
    "def compute_metrics(batch):\n",
    "    label_ids = batch.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_ids = batch.predictions\n",
    "    \n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [remove_punctuation(x).lower().strip() for x in pred_str]\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    return {\"wer\": wer_score}\n",
    "\n",
    "# Evaluate WER\n",
    "def compute_wer(batch, language=\"swahili\"):\n",
    "    inputs = {key: batch[key].to(\"cuda\") for key in batch if key != \"audio\" and key != \"sentence\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ids = model.generate(inputs[\"input_features\"], language=language)\n",
    "    \n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [remove_punctuation(x).lower().strip() for x in pred_str]\n",
    "    label_str = processor.batch_decode(batch[\"labels\"].cpu().numpy(), skip_special_tokens=True)\n",
    "    \n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    return wer_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6162c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"group_idx\"] = torch.LongTensor([feature[\"group_idx\"] for feature in features])\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2b6fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30a06bc5ff94d1f8f4a54f016b6ae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567b814d66e241a1ace72e9b66519e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LANG = \"swahili\"\n",
    "MODEL_ID = \"openai/whisper-tiny\"\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_ID, language=LANG)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(\"cuda\")\n",
    "\n",
    "languages = ['ssw', 'xho', 'swa']\n",
    "\n",
    "# Prepare the data for Hugging Face datasets\n",
    "train_data = {\n",
    "    \"audio\": [],\n",
    "    \"sentence\": [],\n",
    "    \"group_idx\": []\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    \"audio\": [],\n",
    "    \"sentence\": [],\n",
    "    \"group_idx\": []\n",
    "}\n",
    "\n",
    "group_str_to_idx = {\n",
    "    \"ssw\" : 0,\n",
    "    \"xho\" : 1,\n",
    "    \"swa\" : 2,\n",
    "}\n",
    "\n",
    "for language in languages: \n",
    "    for path, sentence in zip(all_paths[\"10mintrain\"][language], all_sentences[\"10mintrain\"][language]):\n",
    "        train_data[\"audio\"].append(path)\n",
    "        train_data[\"sentence\"].append(sentence)\n",
    "        train_data[\"group_idx\"].append(group_str_to_idx[language])\n",
    "    for path, sentence in zip(all_paths[\"10mintest\"][language], all_sentences[\"10mintest\"][language]):\n",
    "        test_data[\"audio\"].append(path)\n",
    "        test_data[\"sentence\"].append(sentence)\n",
    "        test_data[\"group_idx\"].append(group_str_to_idx[language])\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "train_set = train_dataset.map(preprocess, batched=True, batch_size=32).with_format(\"torch\")\n",
    "test_set = test_dataset.map(preprocess, batched=True, batch_size=32).with_format(\"torch\")\n",
    "train_dataloader = DataLoader(train_set, batch_size=32)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bdf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossComputer:\n",
    "    def __init__(self, criterion, is_robust, n_groups, group_counts, group_str_fn, alpha=None, gamma=0.1, adj=None, min_var_weight=0, step_size=0.01, normalize_loss=False, btl=False):\n",
    "        self.criterion = criterion\n",
    "        self.is_robust = is_robust\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.min_var_weight = min_var_weight\n",
    "        self.step_size = step_size\n",
    "        self.normalize_loss = normalize_loss\n",
    "        self.btl = btl\n",
    "\n",
    "        self.n_groups = n_groups\n",
    "        self.group_counts = group_counts.cuda()\n",
    "        self.group_frac = self.group_counts/self.group_counts.sum()\n",
    "        self.group_str = group_str_fn\n",
    "\n",
    "        if adj is not None:\n",
    "            self.adj = torch.from_numpy(adj).float().cuda()\n",
    "        else:\n",
    "            self.adj = torch.zeros(self.n_groups).float().cuda()\n",
    "\n",
    "        if is_robust:\n",
    "            assert alpha, 'alpha must be specified'\n",
    "\n",
    "        # quantities maintained throughout training\n",
    "        self.adv_probs = torch.ones(self.n_groups).cuda()/self.n_groups\n",
    "        self.exp_avg_loss = torch.zeros(self.n_groups).cuda()\n",
    "        self.exp_avg_initialized = torch.zeros(self.n_groups).byte().cuda()\n",
    "\n",
    "        self.reset_stats()\n",
    "\n",
    "    def loss(self, yhat, y, group_idx=None, is_training=False):\n",
    "        # compute per-sample and per-group losses\n",
    "#         print(yhat.shape, y.shape)\n",
    "        per_sample_losses = self.criterion(yhat, y).mean(1)\n",
    "        group_loss, group_count = self.compute_group_avg(per_sample_losses, group_idx)\n",
    "        group_acc, group_count = self.compute_group_avg((torch.argmax(yhat,1)==y).float().mean(1), group_idx)\n",
    "\n",
    "        # update historical losses\n",
    "        self.update_exp_avg_loss(group_loss, group_count)\n",
    "\n",
    "        # compute overall loss\n",
    "        if self.is_robust and not self.btl:\n",
    "            actual_loss, weights = self.compute_robust_loss(group_loss, group_count)\n",
    "        elif self.is_robust and self.btl:\n",
    "             actual_loss, weights = self.compute_robust_loss_btl(group_loss, group_count)\n",
    "        else:\n",
    "            actual_loss = per_sample_losses.mean()\n",
    "            weights = None\n",
    "\n",
    "        # update stats\n",
    "        self.update_stats(actual_loss, group_loss, group_acc, group_count, weights)\n",
    "\n",
    "        return actual_loss\n",
    "\n",
    "    def compute_robust_loss(self, group_loss, group_count):\n",
    "        adjusted_loss = group_loss\n",
    "        if torch.all(self.adj>0):\n",
    "            adjusted_loss += self.adj/torch.sqrt(self.group_counts)\n",
    "        if self.normalize_loss:\n",
    "            adjusted_loss = adjusted_loss/(adjusted_loss.sum())\n",
    "        self.adv_probs = self.adv_probs * torch.exp(self.step_size*adjusted_loss.data)\n",
    "        self.adv_probs = self.adv_probs/(self.adv_probs.sum())\n",
    "\n",
    "        robust_loss = group_loss @ self.adv_probs\n",
    "        return robust_loss, self.adv_probs\n",
    "\n",
    "    def compute_robust_loss_btl(self, group_loss, group_count):\n",
    "        adjusted_loss = self.exp_avg_loss + self.adj/torch.sqrt(self.group_counts)\n",
    "        return self.compute_robust_loss_greedy(group_loss, adjusted_loss)\n",
    "\n",
    "    def compute_robust_loss_greedy(self, group_loss, ref_loss):\n",
    "        sorted_idx = ref_loss.sort(descending=True)[1]\n",
    "        sorted_loss = group_loss[sorted_idx]\n",
    "        sorted_frac = self.group_frac[sorted_idx]\n",
    "\n",
    "        mask = torch.cumsum(sorted_frac, dim=0)<=self.alpha\n",
    "        weights = mask.float() * sorted_frac /self.alpha\n",
    "        last_idx = mask.sum()\n",
    "        weights[last_idx] = 1 - weights.sum()\n",
    "        weights = sorted_frac*self.min_var_weight + weights*(1-self.min_var_weight)\n",
    "\n",
    "        robust_loss = sorted_loss @ weights\n",
    "\n",
    "        # sort the weights back\n",
    "        _, unsort_idx = sorted_idx.sort()\n",
    "        unsorted_weights = weights[unsort_idx]\n",
    "        return robust_loss, unsorted_weights\n",
    "\n",
    "    def compute_group_avg(self, losses, group_idx):\n",
    "        # compute observed counts and mean loss for each group\n",
    "#         print(losses.shape, group_idx.shape)\n",
    "        group_map = (group_idx == torch.arange(self.n_groups).unsqueeze(1).long().cuda()).float()\n",
    "        group_count = group_map.sum(1)\n",
    "        group_denom = group_count + (group_count==0).float() # avoid nans\n",
    "        group_loss = (group_map @ losses.view(-1))/group_denom\n",
    "        return group_loss, group_count\n",
    "\n",
    "    def update_exp_avg_loss(self, group_loss, group_count):\n",
    "        prev_weights = (1 - self.gamma*(group_count>0).float()) * (self.exp_avg_initialized>0).float()\n",
    "        curr_weights = 1 - prev_weights\n",
    "        self.exp_avg_loss = self.exp_avg_loss * prev_weights + group_loss*curr_weights\n",
    "        self.exp_avg_initialized = (self.exp_avg_initialized>0) + (group_count>0)\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.processed_data_counts = torch.zeros(self.n_groups).cuda()\n",
    "        self.update_data_counts = torch.zeros(self.n_groups).cuda()\n",
    "        self.update_batch_counts = torch.zeros(self.n_groups).cuda()\n",
    "        self.avg_group_loss = torch.zeros(self.n_groups).cuda()\n",
    "        self.avg_group_acc = torch.zeros(self.n_groups).cuda()\n",
    "        self.avg_per_sample_loss = 0.\n",
    "        self.avg_actual_loss = 0.\n",
    "        self.avg_acc = 0.\n",
    "        self.batch_count = 0.\n",
    "\n",
    "    def update_stats(self, actual_loss, group_loss, group_acc, group_count, weights=None):\n",
    "        # avg group loss\n",
    "        denom = self.processed_data_counts + group_count\n",
    "        denom += (denom==0).float()\n",
    "        prev_weight = self.processed_data_counts/denom\n",
    "        curr_weight = group_count/denom\n",
    "        self.avg_group_loss = prev_weight*self.avg_group_loss + curr_weight*group_loss\n",
    "\n",
    "        # avg group acc\n",
    "        self.avg_group_acc = prev_weight*self.avg_group_acc + curr_weight*group_acc\n",
    "\n",
    "        # batch-wise average actual loss\n",
    "        denom = self.batch_count + 1\n",
    "        self.avg_actual_loss = (self.batch_count/denom)*self.avg_actual_loss + (1/denom)*actual_loss\n",
    "\n",
    "        # counts\n",
    "        self.processed_data_counts += group_count\n",
    "        if self.is_robust:\n",
    "            self.update_data_counts += group_count*((weights>0).float())\n",
    "            self.update_batch_counts += ((group_count*weights)>0).float()\n",
    "        else:\n",
    "            self.update_data_counts += group_count\n",
    "            self.update_batch_counts += (group_count>0).float()\n",
    "        self.batch_count+=1\n",
    "\n",
    "        # avg per-sample quantities\n",
    "        group_frac = self.processed_data_counts/(self.processed_data_counts.sum())\n",
    "        self.avg_per_sample_loss = group_frac @ self.avg_group_loss\n",
    "        self.avg_acc = group_frac @ self.avg_group_acc\n",
    "\n",
    "    def get_model_stats(self, model, args, stats_dict):\n",
    "        model_norm_sq = 0.\n",
    "        for param in model.parameters():\n",
    "            model_norm_sq += torch.norm(param) ** 2\n",
    "        stats_dict['model_norm_sq'] = model_norm_sq.item()\n",
    "        stats_dict['reg_loss'] = args.weight_decay / 2 * model_norm_sq.item()\n",
    "        return stats_dict\n",
    "\n",
    "    def get_stats(self, model=None, args=None):\n",
    "        stats_dict = {}\n",
    "        for idx in range(self.n_groups):\n",
    "            stats_dict[f'avg_loss_group:{idx}'] = self.avg_group_loss[idx].item()\n",
    "            stats_dict[f'exp_avg_loss_group:{idx}'] = self.exp_avg_loss[idx].item()\n",
    "            stats_dict[f'avg_acc_group:{idx}'] = self.avg_group_acc[idx].item()\n",
    "            stats_dict[f'processed_data_count_group:{idx}'] = self.processed_data_counts[idx].item()\n",
    "            stats_dict[f'update_data_count_group:{idx}'] = self.update_data_counts[idx].item()\n",
    "            stats_dict[f'update_batch_count_group:{idx}'] = self.update_batch_counts[idx].item()\n",
    "\n",
    "        stats_dict['avg_actual_loss'] = self.avg_actual_loss.item()\n",
    "        stats_dict['avg_per_sample_loss'] = self.avg_per_sample_loss.item()\n",
    "        stats_dict['avg_acc'] = self.avg_acc.item()\n",
    "\n",
    "        # Model stats\n",
    "        if model is not None:\n",
    "            assert args is not None\n",
    "            stats_dict = self.get_model_stats(model, args, stats_dict)\n",
    "\n",
    "        return stats_dict\n",
    "\n",
    "    def log_stats(self, logger, is_training):\n",
    "        if logger is None:\n",
    "            return\n",
    "\n",
    "        logger.write(f'Average incurred loss: {self.avg_per_sample_loss.item():.3f}  \\n')\n",
    "        logger.write(f'Average sample loss: {self.avg_actual_loss.item():.3f}  \\n')\n",
    "        logger.write(f'Average acc: {self.avg_acc.item():.3f}  \\n')\n",
    "        for group_idx in range(self.n_groups):\n",
    "            logger.write(\n",
    "                f'  {self.group_str(group_idx)}  '\n",
    "                f'[n = {int(self.processed_data_counts[group_idx])}]:\\t'\n",
    "                f'loss = {self.avg_group_loss[group_idx]:.3f}  '\n",
    "                f'exp loss = {self.exp_avg_loss[group_idx]:.3f}  '\n",
    "                f'adjusted loss = {self.exp_avg_loss[group_idx] + self.adj[group_idx]/torch.sqrt(self.group_counts)[group_idx]:.3f}  '\n",
    "                f'adv prob = {self.adv_probs[group_idx]:3f}   '\n",
    "                f'acc = {self.avg_group_acc[group_idx]:.3f}\\n')\n",
    "        logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87e3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_str_fn(group_idx):\n",
    "    x = {\n",
    "        0: \"ssw\",\n",
    "        1: \"xho\",\n",
    "        2: \"tsn\"\n",
    "    }\n",
    "\n",
    "    return x[group_idx]\n",
    "\n",
    "\n",
    "train_loss_computer = LossComputer(torch.nn.CrossEntropyLoss(reduction='none'), True, 3, torch.Tensor([5,5,5]), group_str_fn, alpha=0.2)\n",
    "val_loss_computer = LossComputer(torch.nn.CrossEntropyLoss(reduction='none'), True, 3, torch.Tensor([5,5,5]), group_str_fn, alpha=0.2)\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    inputs_no_idx = {k: v for k, v in inputs.items() if k != \"group_idx\"}\n",
    "    outputs = model(**inputs_no_idx).logits\n",
    "    if model.training:\n",
    "        return train_loss_computer.loss(outputs.permute(0,2,1), inputs[\"labels\"], inputs[\"group_idx\"])\n",
    "    else:\n",
    "        return val_loss_computer.loss(outputs.permute(0,2,1), inputs[\"labels\"], inputs[\"group_idx\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ed0ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_step(\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        **gen_kwargs,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            gen_kwargs:\n",
    "                Additional `generate` specific kwargs.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
    "            labels (each being optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.args.predict_with_generate or prediction_loss_only:\n",
    "            return super().prediction_step(\n",
    "                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n",
    "            )\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # Priority (handled in generate):\n",
    "        # non-`None` gen_kwargs > model.generation_config > default GenerationConfig()\n",
    "        if len(gen_kwargs) == 0 and hasattr(self, \"_gen_kwargs\"):\n",
    "            gen_kwargs = self._gen_kwargs.copy()\n",
    "        if \"num_beams\" in gen_kwargs and gen_kwargs[\"num_beams\"] is None:\n",
    "            gen_kwargs.pop(\"num_beams\")\n",
    "        if \"max_length\" in gen_kwargs and gen_kwargs[\"max_length\"] is None:\n",
    "            gen_kwargs.pop(\"max_length\")\n",
    "\n",
    "        default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n",
    "        gen_kwargs[\"synced_gpus\"] = (\n",
    "            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n",
    "        )\n",
    "\n",
    "        generation_inputs = inputs.copy()\n",
    "        # If the `decoder_input_ids` was created from `labels`, evict the former, so that the model can freely generate\n",
    "        # (otherwise, it would continue generating from the padded `decoder_input_ids`)\n",
    "        if (\n",
    "            \"labels\" in generation_inputs\n",
    "            and \"decoder_input_ids\" in generation_inputs\n",
    "            and generation_inputs[\"labels\"].shape == generation_inputs[\"decoder_input_ids\"].shape\n",
    "        ):\n",
    "            generation_inputs = {\n",
    "                k: v for k, v in inputs.items() if k not in (\"decoder_input_ids\", \"decoder_attention_mask\")\n",
    "            }\n",
    "        generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
    "\n",
    "        # Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\n",
    "        # TODO: remove this hack when the legacy code that initializes generation_config from a model config is\n",
    "        # removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\n",
    "        if self.model.generation_config._from_model_config:\n",
    "            self.model.generation_config._from_model_config = False\n",
    "\n",
    "        # Retrieves GenerationConfig from model.generation_config\n",
    "        gen_config = self.model.generation_config\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_config.max_length:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_length)\n",
    "        elif gen_config.max_new_tokens is not None and generated_tokens.shape[-1] < gen_config.max_new_tokens + 1:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_new_tokens + 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                with self.compute_loss_context_manager():\n",
    "                    inputs_no_idx = {k: v for k, v in inputs.items() if k != \"group_idx\"}\n",
    "                    outputs = model(**inputs_no_idx).logits\n",
    "                loss = val_loss_computer.loss(outputs.permute(0,2,1), inputs[\"labels\"], inputs[\"group_idx\"])\n",
    "#                 elif self.label_smoother is not None:\n",
    "#                     loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
    "#                 else:\n",
    "#                     loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "        if self.args.prediction_loss_only:\n",
    "            return loss, None, None\n",
    "\n",
    "        if has_labels:\n",
    "            labels = inputs[\"labels\"]\n",
    "            if labels.shape[-1] < gen_config.max_length:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_config.max_length)\n",
    "            elif gen_config.max_new_tokens is not None and labels.shape[-1] < gen_config.max_new_tokens + 1:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_config.max_new_tokens + 1)\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return loss, generated_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a265a5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=50,\n",
    "    max_steps=300,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1,\n",
    "    logging_steps=100,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21ed5ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50207b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._signature_columns =['input_features',\n",
    " 'attention_mask',\n",
    " 'decoder_input_ids',\n",
    " 'decoder_attention_mask',\n",
    " 'head_mask',\n",
    " 'decoder_head_mask',\n",
    " 'cross_attn_head_mask',\n",
    " 'encoder_outputs',\n",
    " 'past_key_values',\n",
    " 'decoder_inputs_embeds',\n",
    " 'decoder_position_ids',\n",
    " 'labels',\n",
    " 'use_cache',\n",
    " 'output_attentions',\n",
    " 'output_hidden_states',\n",
    " 'return_dict',\n",
    " 'labels',\n",
    " 'label_ids',\n",
    " 'label',\n",
    "  'group_idx']\n",
    "\n",
    "# /viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1542\n",
    "# comment out this line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2de9f01f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/300 : < :, Epoch 0.08/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcompute_loss \u001b[38;5;241m=\u001b[39m compute_loss\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mprediction_step \u001b[38;5;241m=\u001b[39m prediction_step\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:3757\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3754\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3757\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3758\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3759\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m, in \u001b[0;36mprediction_step\u001b[0;34m(model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction_step\u001b[39m(\n\u001b[1;32m      2\u001b[0m         model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m      3\u001b[0m         inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs,\n\u001b[1;32m      7\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[\u001b[38;5;28mfloat\u001b[39m], Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        Perform an evaluation step on `model` using `inputs`.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m            labels (each being optional).\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpredict_with_generate \u001b[38;5;129;01mor\u001b[39;00m prediction_loss_only:\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprediction_step(\n\u001b[1;32m     33\u001b[0m                 model, inputs, prediction_loss_only\u001b[38;5;241m=\u001b[39mprediction_loss_only, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys\n\u001b[1;32m     34\u001b[0m             )\n\u001b[1;32m     36\u001b[0m         has_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.compute_loss = compute_loss\n",
    "trainer.prediction_step = prediction_step\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5198f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = LANG\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Compute WER for the entire dataset\n",
    "wer_scores = []\n",
    "\n",
    "# Use tqdm to wrap your dataloader to show a progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "    wer_scores.append(compute_wer(batch, language=LANG))\n",
    "\n",
    "average_wer = np.mean(wer_scores)\n",
    "print(f\"{language} Average WER: {average_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f41333a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    inputs = {key: batch[key].to(\"cuda\") for key in batch if key != \"audio\" and key != \"sentence\"}\n",
    "    model.generation_config.language = \"swahili\"\n",
    "    model.generation_config.task = \"transcribe\"\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ids = model.generate(inputs[\"input_features\"], language=\"swahili\", )\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [remove_punctuation(x).lower().strip() for x in pred_str]\n",
    "    label_str = processor.batch_decode(batch[\"labels\"].cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f56e8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "17043214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mbunji inum blonganjin'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_str[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81acd368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mbÃ´nji i nnumb loÃ±ge njiÃ±'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_str[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4b809adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer([label_str[i]], [pred_str[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "710fa0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_str_fn(group_idx):\n",
    "    x = {\n",
    "        0: \"ssw\",\n",
    "        1: \"xho\",\n",
    "        2: \"tsn\"\n",
    "    }\n",
    "\n",
    "    return x[group_idx]\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# y_hat = torch.rand((8, 32)).cuda()\n",
    "# y = torch.randint(8,32,(8,)).cuda().long()\n",
    "# group_idxs = torch.Tensor([0,0,1,2,1,0,1,0]).cuda()\n",
    "# # group_idxs = torch.Tensor([0,0,0,0,0,0,0,0]).cuda()\n",
    "\n",
    "\n",
    "# print(loss_computer.loss(y_hat, y, group_idxs))\n",
    "\n",
    "# y_hat = torch.rand((8, 32)).cuda()\n",
    "# y = torch.randint(8,32,(8,)).cuda().long()\n",
    "# group_idxs = torch.Tensor([0,0,1,0,1,0,1,0]).cuda()\n",
    "# # group_idxs = torch.Tensor([0,0,0,0,0,0,0,0]).cuda()\n",
    "\n",
    "\n",
    "# print(loss_computer.loss(y_hat, y, group_idxs))\n",
    "\n",
    "train_loss_computer = LossComputer(torch.nn.CrossEntropyLoss(reduction='none'), True, 3, torch.Tensor([5,5,5]), group_str_fn, alpha=0.2)\n",
    "val_loss_computer = LossComputer(torch.nn.CrossEntropyLoss(reduction='none'), True, 3, torch.Tensor([5,5,5]), group_str_fn, alpha=0.2)\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    inputs_no_idx = {k: v for k, v in inputs.items() if k != \"group_idx\"}\n",
    "    outputs = model(**inputs_no_idx).logits\n",
    "    if model.training:\n",
    "        return train_loss_computer.loss(outputs.permute(0,2,1), inputs[\"labels\"], inputs[\"group_idx\"])\n",
    "    else:\n",
    "        return val_loss_computer.loss(outputs.permute(0,2,1), inputs[\"labels\"], inputs[\"group_idx\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2552b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
