{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e533aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets \n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from safetensors import safe_open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6ae418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/vision/u/eatang/ml_superb/eighth_version/'\n",
    "languages = []\n",
    "\n",
    "sources = os.listdir(dataset_path)\n",
    "\n",
    "for source in sources:\n",
    "    if source[0] != \".\":\n",
    "        languages.extend(os.listdir(os.path.join(dataset_path, source)))\n",
    "        \n",
    "languages = set([x for x in languages if '.' not in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea5a2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "all_paths = {}\n",
    "all_sentences = {}\n",
    "for duration in [\"10min\", \"1h\"]:\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        language_to_paths = defaultdict(list)\n",
    "        language_to_sentences = defaultdict(list)\n",
    "        for language in languages:\n",
    "            for source in sources:\n",
    "                source_lang_path = os.path.join(dataset_path, source, language)\n",
    "                if os.path.exists(os.path.join(source_lang_path, f'transcript_{duration}_{split}.txt')):\n",
    "                    with open(os.path.join(source_lang_path, f'transcript_{duration}_{split}.txt'), 'r') as file:\n",
    "                        lines = [line.rstrip() for line in file]\n",
    "                        sentences = []\n",
    "                        paths = []\n",
    "                        for line in lines:\n",
    "                            sentence = \" \".join(re.split(r'[ \\t]+', line)[2:])\n",
    "                            sentence = remove_punctuation(sentence).lower().strip()\n",
    "                            if len(sentence) <= 1:\n",
    "                                continue\n",
    "                            if len(re.split(r'[ \\t]+', line)[0]) > 0:\n",
    "                                sentences.append(sentence)\n",
    "                                paths.append(os.path.join(source_lang_path, 'wav', re.split(r'[ \\t]+', line)[0] + '.wav'))\n",
    "\n",
    "                        language_to_paths[language].extend(paths)\n",
    "                        language_to_sentences[language].extend(sentences)\n",
    "        all_paths[duration + split] = language_to_paths\n",
    "        all_sentences[duration + split] = language_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca382554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor,WhisperForConditionalGeneration, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from jiwer import wer\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_audio(path):\n",
    "    speech, _ = torchaudio.load(path)\n",
    "    return speech.squeeze().numpy()\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(batch):\n",
    "    audio = [load_audio(path) for path in batch[\"audio\"]]\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\").to(\"cuda\")\n",
    "    labels = processor.tokenizer(text=batch[\"sentence\"], return_tensors=\"pt\", padding=True).input_ids\n",
    "    inputs[\"labels\"] = labels.to(\"cuda\")\n",
    "    return inputs\n",
    "\n",
    "# Function to decode model predictions\n",
    "def decode_predictions(pred_ids):\n",
    "    pred_ids = pred_ids.cpu().numpy()\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    return pred_str\n",
    "\n",
    "# Evaluate WER\n",
    "def compute_metrics(batch):\n",
    "    label_ids = batch.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_ids = batch.predictions\n",
    "    \n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [remove_punctuation(x).lower().strip() for x in pred_str]\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    return {\"wer\": wer_score}\n",
    "\n",
    "# Evaluate WER\n",
    "def compute_wer(batch, language=LANG):\n",
    "    inputs = {key: batch[key].to(\"cuda\") for key in batch if key != \"audio\" and key != \"sentence\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ids = model.generate(inputs[\"input_features\"], language=language)\n",
    "    \n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [remove_punctuation(x).lower().strip() for x in pred_str]\n",
    "    label_str = processor.batch_decode(batch[\"labels\"].cpu().numpy(), skip_special_tokens=True)\n",
    "    \n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    return wer_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6162c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5a2b6fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3480d8543aa04a49baf4bab62917d8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssw Average WER: 0.5841399899541198\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88595553b6044fb7a175fb4f458cc5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swa Average WER: 0.5750793521499591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9fb15481db40c981629877ce685185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xho Average WER: 0.6668938540877771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LANG = \"swahili\"\n",
    "# MODEL_ID = \"openai/whisper-tiny\"\n",
    "# processor = WhisperProcessor.from_pretrained(MODEL_ID, language=LANG)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID).to(\"cuda\")\n",
    "\n",
    "languages = ['ssw', 'swa', 'xho']\n",
    "\n",
    "# Prepare the data for Hugging Face datasets\n",
    "train_data = {\n",
    "    \"audio\": [],\n",
    "    \"sentence\": []\n",
    "}\n",
    "\n",
    "for language in languages: \n",
    "    test_data = {\n",
    "        \"audio\": [],\n",
    "        \"sentence\": []\n",
    "    }\n",
    "#     for path, sentence in zip(all_paths[\"1htrain\"][language], all_sentences[\"1htrain\"][language]):\n",
    "#         train_data[\"audio\"].append(path)\n",
    "#         train_data[\"sentence\"].append(sentence)\n",
    "    for path, sentence in zip(all_paths[\"10mintest\"][language], all_sentences[\"10mintest\"][language]):\n",
    "        test_data[\"audio\"].append(path)\n",
    "        test_data[\"sentence\"].append(sentence)\n",
    "\n",
    "    # test_data[\"audio\"] = test_data[\"audio\"][:32]\n",
    "    # test_data[\"sentence\"] = test_data[\"sentence\"][:32]\n",
    "\n",
    "    # Create a Hugging Face dataset\n",
    "#     train_dataset = Dataset.from_dict(train_data)\n",
    "    test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "#     train_set = train_dataset.map(preprocess, batched=True, batch_size=32).with_format(\"torch\")\n",
    "    test_set = test_dataset.map(preprocess, batched=True, batch_size=32).with_format(\"torch\")\n",
    "#     train_dataloader = DataLoader(train_set, batch_size=32)\n",
    "    test_dataloader = DataLoader(test_set, batch_size=32)\n",
    "    \n",
    "    model.generation_config.language = LANG\n",
    "    model.generation_config.task = \"transcribe\"\n",
    "    model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "    # Compute WER for the entire dataset\n",
    "    wer_scores = []\n",
    "\n",
    "    # Use tqdm to wrap your dataloader to show a progress bar\n",
    "    for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "        wer_scores.append(compute_wer(batch, language=LANG))\n",
    "\n",
    "    average_wer = np.mean(wer_scores)\n",
    "    print(f\"{language} Average WER: {average_wer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a265a5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/viscam/u/eatang/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=50,\n",
    "    max_steps=300,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e0c54ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a9f1e8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 09:41, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.662768</td>\n",
       "      <td>0.725646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.570663</td>\n",
       "      <td>0.610344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.559565</td>\n",
       "      <td>0.605384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6795074792702993, metrics={'train_runtime': 582.7457, 'train_samples_per_second': 32.947, 'train_steps_per_second': 0.515, 'total_flos': 4.692359503872e+17, 'train_loss': 0.6795074792702993, 'epoch': 4.285714285714286})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f5198f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [00:38<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xho Average WER: 0.6248699750350295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.generation_config.language = LANG\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Compute WER for the entire dataset\n",
    "wer_scores = []\n",
    "\n",
    "# Use tqdm to wrap your dataloader to show a progress bar\n",
    "for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "    wer_scores.append(compute_wer(batch, language=LANG))\n",
    "\n",
    "average_wer = np.mean(wer_scores)\n",
    "print(f\"{language} Average WER: {average_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f41333a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    inputs = {key: batch[key].to(\"cuda\") for key in batch if key != \"audio\" and key != \"sentence\"}\n",
    "    model.generation_config.language = \"swahili\"\n",
    "    model.generation_config.task = \"transcribe\"\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_ids = model.generate(inputs[\"input_features\"], language=\"swahili\", )\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [remove_punctuation(x).lower().strip() for x in pred_str]\n",
    "    label_str = processor.batch_decode(batch[\"labels\"].cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f56e8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "17043214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mbunji inum blonganjin'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_str[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81acd368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mbônji i nnumb loñge njiñ'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_str[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4b809adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer([label_str[i]], [pred_str[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fa0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
